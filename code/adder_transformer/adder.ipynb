{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "  data = []\n",
    "  for i in range(100):\n",
    "    for j in range(100):\n",
    "      s = i + j\n",
    "      data.append([i // 10, i % 10, j // 10, j % 10, s // 100, (s // 10) % 10, s % 10])\n",
    "  random.shuffle(data)\n",
    "  data = Tensor(data)\n",
    "  X_train = data[:8000, :-1]\n",
    "  Y_train = data[:8000, 1:]\n",
    "  X_test = data[8000:, :-1]\n",
    "  Y_test = data[8000:, 1:]\n",
    "  return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Tensor <LB METAL (8000, 6) int ShapeTracker(views=(View(shape=(8000, 6), strides=(7, 1), offset=0, mask=None, contiguous=False),))> on METAL with grad None>,\n",
       " <Tensor <LB METAL (8000, 6) int ShapeTracker(views=(View(shape=(8000, 6), strides=(7, 1), offset=1, mask=None, contiguous=False),))> on METAL with grad None>,\n",
       " <Tensor <LB METAL (2000, 6) int ShapeTracker(views=(View(shape=(2000, 6), strides=(7, 1), offset=56000, mask=None, contiguous=False),))> on METAL with grad None>,\n",
       " <Tensor <LB METAL (2000, 6) int ShapeTracker(views=(View(shape=(2000, 6), strides=(7, 1), offset=56001, mask=None, contiguous=False),))> on METAL with grad None>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = build_dataset()\n",
    "\n",
    "X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, n_heads, embed_size) -> None:\n",
    "    self.n_heads = n_heads\n",
    "    self.internal_embed_size = embed_size * 2\n",
    "    bound = 1 / math.sqrt(self.internal_embed_size)\n",
    "    self.keys = Tensor.uniform(\n",
    "      n_heads, embed_size, self.internal_embed_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.queries = Tensor.uniform(\n",
    "      n_heads, embed_size, self.internal_embed_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.values = Tensor.uniform(\n",
    "      n_heads, embed_size, self.internal_embed_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.linear = Tensor.uniform(\n",
    "      n_heads * self.internal_embed_size, embed_size, low=-bound, high=bound\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    B, T, C = x.shape\n",
    "\n",
    "    x = x.unsqueeze(1).expand((B, self.n_heads, T, C))\n",
    "\n",
    "    K = x @ self.keys\n",
    "    Q = x @ self.queries\n",
    "    V = x @ self.values\n",
    "\n",
    "    # dot_attn = Q @ K.transpose(-2, -1)\n",
    "    # scaled_dot_attn = dot_attn / math.sqrt(self.attn_embed_size)\n",
    "    # masked_scaled_dot_attn = scaled_dot_attn#.tril().where(scaled_dot_attn, float(\"-inf\"))\n",
    "    # attn_scores = masked_scaled_dot_attn.softmax()\n",
    "\n",
    "    # ret = attn_scores @ V\n",
    "    # return ret\n",
    "\n",
    "    ret = Tensor.scaled_dot_product_attention(\n",
    "      K, Q, V, attn_mask=Tensor.ones((T, T)).tril()\n",
    "    )\n",
    "    ret = ret.reshape((B, T, self.n_heads * self.internal_embed_size))\n",
    "    ret = ret @ self.linear\n",
    "    ret = ret.gelu()\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, vocab_size, embed_size, n_layers, n_heads) -> None:\n",
    "    self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.h = [Attention(n_heads, embed_size) for _ in range(n_layers)]\n",
    "    self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "  def forward(self, x: Tensor) -> Tensor:\n",
    "    logits = x.sequential([self.token_embed, *self.h, self.linear])\n",
    "    return logits\n",
    "\n",
    "  def loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "    logits = self.forward(x)\n",
    "    loss = logits.sparse_categorical_crossentropy(y)\n",
    "    return logits, loss\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    logits = self.forward(x)\n",
    "    return logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526858"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(vocab_size=10, embed_size=128, n_layers=2, n_heads=2)\n",
    "sum(p.numel() for p in nn.state.get_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.AdamW(nn.state.get_parameters(model))\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "  optim.zero_grad()\n",
    "  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "  X_samples, Y_samples = X_train[samples], Y_train[samples]\n",
    "  _, loss = model.loss(X_samples, Y_samples)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 2.30, acc 9.45%\n",
      "step 250, loss 0.59, acc 21.40%\n",
      "step 500, loss 0.27, acc 67.45%\n",
      "step 750, loss 0.11, acc 91.10%\n",
      "step 1000, loss 0.03, acc 97.15%\n",
      "step 1250, loss 0.03, acc 98.10%\n",
      "step 1500, loss 0.01, acc 98.65%\n",
      "step 1750, loss 0.02, acc 98.75%\n",
      "step 2000, loss 0.01, acc 98.75%\n",
      "step 2250, loss 0.00, acc 98.95%\n",
      "step 2500, loss 0.01, acc 98.70%\n",
      "step 2750, loss 0.02, acc 97.95%\n",
      "step 3000, loss 0.01, acc 99.55%\n",
      "step 3250, loss 0.00, acc 99.65%\n",
      "step 3500, loss 0.03, acc 99.35%\n",
      "step 3750, loss 0.00, acc 99.35%\n",
      "step 4000, loss 0.00, acc 99.10%\n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 4001):\n",
    "  loss = train_step()\n",
    "  if step == 1 or step % 250 == 0:\n",
    "    with Tensor.inference_mode():\n",
    "      acc = (model(X_test).argmax(axis=-1) == Y_test[:, -1]).mean().item()\n",
    "      print(f\"step {step}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: [1, 0, 0, 0, 0, 1], Prediction: 1, True: 0\n",
      "Example: [0, 3, 2, 1, 0, 2], Prediction: 3, True: 4\n",
      "Example: [0, 3, 2, 0, 0, 2], Prediction: 2, True: 3\n",
      "Example: [4, 0, 0, 0, 0, 4], Prediction: 4, True: 0\n",
      "Example: [9, 0, 0, 0, 0, 9], Prediction: 9, True: 0\n",
      "Example: [8, 7, 0, 0, 0, 8], Prediction: 5, True: 7\n",
      "Example: [8, 0, 0, 0, 0, 8], Prediction: 8, True: 0\n",
      "Example: [0, 9, 3, 0, 0, 3], Prediction: 0, True: 9\n",
      "Example: [9, 6, 0, 3, 0, 9], Prediction: 1, True: 9\n",
      "Example: [0, 0, 8, 0, 0, 8], Prediction: 8, True: 0\n",
      "Example: [8, 8, 0, 0, 0, 8], Prediction: 6, True: 8\n",
      "Example: [0, 0, 0, 0, 0, 0], Prediction: 7, True: 0\n",
      "Example: [0, 0, 0, 3, 0, 0], Prediction: 7, True: 3\n",
      "Example: [0, 3, 5, 0, 0, 5], Prediction: 2, True: 3\n",
      "Example: [0, 9, 4, 1, 0, 5], Prediction: 1, True: 0\n",
      "Example: [1, 7, 0, 0, 0, 1], Prediction: 8, True: 7\n",
      "Example: [0, 3, 0, 1, 0, 0], Prediction: 3, True: 4\n",
      "Example: [3, 0, 9, 3, 1, 2], Prediction: 2, True: 3\n"
     ]
    }
   ],
   "source": [
    "predictions = model(X_test).argmax(axis=-1)\n",
    "true_labels = Y_test[:, -1]\n",
    "incorrect_mask = predictions != true_labels\n",
    "incorrect_indices = np.where(incorrect_mask.numpy())[0].tolist()\n",
    "\n",
    "for index in incorrect_indices:\n",
    "  print(\n",
    "    f\"Example: {X_test[index].tolist()}, Prediction: {predictions[index].item()}, True: {true_labels[index].item()}\"\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
