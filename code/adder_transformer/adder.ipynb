{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "  data = []\n",
    "  for i in range(100):\n",
    "    for j in range(100):\n",
    "      s = i + j\n",
    "      data.append([i // 10, i % 10, j // 10, j % 10, s // 100, (s // 10) % 10, s % 10])\n",
    "  np.random.shuffle(data)\n",
    "  data = Tensor(data)\n",
    "  X_train = data[:8000, :-1]\n",
    "  Y_train = data[:8000, 1:]\n",
    "  X_test = data[8000:, :-1]\n",
    "  Y_test = data[8000:, 1:]\n",
    "  return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Tensor <LB METAL (8000, 6) int ShapeTracker(views=(View(shape=(8000, 6), strides=(7, 1), offset=0, mask=None, contiguous=False),))> on METAL with grad None>,\n",
       " <Tensor <LB METAL (8000, 6) int ShapeTracker(views=(View(shape=(8000, 6), strides=(7, 1), offset=1, mask=None, contiguous=False),))> on METAL with grad None>,\n",
       " <Tensor <LB METAL (2000, 6) int ShapeTracker(views=(View(shape=(2000, 6), strides=(7, 1), offset=56000, mask=None, contiguous=False),))> on METAL with grad None>,\n",
       " <Tensor <LB METAL (2000, 6) int ShapeTracker(views=(View(shape=(2000, 6), strides=(7, 1), offset=56001, mask=None, contiguous=False),))> on METAL with grad None>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = build_dataset()\n",
    "\n",
    "X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, embed_size, n_heads, head_size) -> None:\n",
    "    self.n_heads = n_heads\n",
    "    self.head_size = head_size\n",
    "    bound = 1 / (self.head_size**0.5)\n",
    "    self.queries = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.keys = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.values = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    B, T, C = x.shape\n",
    "\n",
    "    x = x.unsqueeze(1).expand((B, self.n_heads, T, C))\n",
    "\n",
    "    Q = x @ self.queries  # (B, n_heads, T, head_size)\n",
    "    K = x @ self.keys  # (B, n_heads, T, head_size)\n",
    "    dot_attn = Q @ K.transpose(-2, -1)  # (B, n_heads, T, T)\n",
    "    scaled_dot_attn: Tensor = dot_attn / (self.head_size**0.5)  # (B, n_heads, T, T)\n",
    "    mask = Tensor.ones((T, T), requires_grad=False).tril()  # (T, T)\n",
    "    masked_scaled_dot_attn = scaled_dot_attn.masked_fill(mask == 0, float(\"-inf\"))  # noqa: F401, (B, n_heads, T, T)\n",
    "    attn_scores = masked_scaled_dot_attn.softmax()  # (B, n_heads, T, T)\n",
    "\n",
    "    V = x @ self.values  # (B, n_heads, T, head_size)\n",
    "    attented_embeds = attn_scores @ V  # (B, n_heads, T, head_size)\n",
    "    concatenated_embeds = attented_embeds.reshape((B, T, self.n_heads * self.head_size))  # noqa: F401, (B, T, n_heads * head_size)\n",
    "    return concatenated_embeds\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "    self.attn = Attention(embed_size, n_heads, head_size)\n",
    "    self.out_proj = nn.Linear(n_heads * head_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    return x.sequential([self.attn, self.out_proj, Tensor.gelu])\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(self, vocab_size, embed_size, n_layers, n_heads, head_size) -> None:\n",
    "    self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.h = [TransformerBlock(embed_size, n_heads, head_size) for _ in range(n_layers)]\n",
    "    self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "  def forward(self, x: Tensor) -> Tensor:\n",
    "    logits = x.sequential([self.token_embed, *self.h, self.lm_head])\n",
    "    return logits\n",
    "\n",
    "  def loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "    logits = self.forward(x)\n",
    "    loss = logits.sparse_categorical_crossentropy(y)\n",
    "    return logits, loss\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    logits = self.forward(x)\n",
    "    return logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2108426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 512\n",
    "n_heads = 4\n",
    "head_size = embed_size // n_heads\n",
    "model = Transformer(\n",
    "  vocab_size=10, embed_size=embed_size, n_layers=2, n_heads=n_heads, head_size=head_size\n",
    ")\n",
    "sum(p.numel() for p in nn.state.get_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.AdamW(nn.state.get_parameters(model))\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "  optim.zero_grad()\n",
    "  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "  X_samples, Y_samples = X_train[samples], Y_train[samples]\n",
    "  _, loss = model.loss(X_samples, Y_samples)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 2.30, acc 9.85%\n",
      "step 250, loss 0.01, acc 98.70%\n",
      "step 500, loss 0.02, acc 99.50%\n",
      "step 750, loss 0.00, acc 100.00%\n",
      "step 1000, loss 0.00, acc 100.00%\n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 1001):\n",
    "  loss = train_step()\n",
    "  if step == 1 or step % 250 == 0:\n",
    "    with Tensor.inference_mode():\n",
    "      acc = (model(X_test).argmax(axis=-1) == Y_test[:, -1]).mean().item()\n",
    "      print(f\"step {step}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: [0, 3, 0, 3, 0, 0], Prediction: 3, True: 6\n",
      "Example: [0, 1, 0, 9, 0, 1], Prediction: 9, True: 0\n",
      "Example: [6, 0, 6, 0, 1, 2], Prediction: 6, True: 0\n",
      "Example: [0, 1, 0, 1, 0, 0], Prediction: 1, True: 2\n",
      "Example: [0, 5, 0, 0, 0, 0], Prediction: 0, True: 5\n"
     ]
    }
   ],
   "source": [
    "predictions = model(X_test).argmax(axis=-1)\n",
    "true_labels = Y_test[:, -1]\n",
    "incorrect_mask = predictions != true_labels\n",
    "incorrect_indices = np.where(incorrect_mask.numpy())[0].tolist()\n",
    "\n",
    "for index in incorrect_indices:\n",
    "  print(\n",
    "    f\"Example: {X_test[index].tolist()}, Prediction: {predictions[index].item()}, True: {true_labels[index].item()}\"\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
