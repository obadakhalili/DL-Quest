{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['JIT'] = '2'\n",
    "Device.DEFAULT = \"GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"shakespeare.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 chars\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), \"chars\")\n",
    "print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, \"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line_char = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = lambda s: [vocab.index(c) for c in s]\n",
    "decode = lambda l: \"\".join([vocab[i] for i in l])\n",
    "\n",
    "decode(encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(encode(corpus))\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data: Tensor, batch_size, block_size):\n",
    "  indices = Tensor.randint((batch_size,), high=len(data) - block_size).reshape(\n",
    "    (batch_size, 1)\n",
    "  ) + Tensor.arange(block_size)\n",
    "  return data[indices], data[indices + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (4, 8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(train_data, batch_size=4, block_size=8)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bolingb', 'ks. What', 'IUS:\\nI t', ' house,\\n']\n",
      "['Bolingbr', \"s. What'\", 'US:\\nI te', 'house,\\nA']\n"
     ]
    }
   ],
   "source": [
    "print([decode(row) for row in x.numpy()])\n",
    "print([decode(row) for row in y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class CausalSelfAttention:\n",
    "  def __init__(self, n_embd: int, n_head:int, block_size:int):\n",
    "    assert n_embd % n_head == 0\n",
    "    # key, query, value projections for all heads, but in a batch\n",
    "    self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "    # regularization\n",
    "    self.n_head = n_head\n",
    "    self.n_embd = n_embd\n",
    "    # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "    self.bias = Tensor.ones(1, 1, block_size, block_size).tril()\n",
    "    self.bias.requires_grad = False\n",
    "\n",
    "  def __call__(self, x:Tensor):\n",
    "    B, T, C = x.shape\n",
    "    qkv = self.c_attn(x)\n",
    "    q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # manual implementation of attention\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = att.softmax()\n",
    "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    y = y.transpose(1, 2).view(B, T, C) # re-assemble all head outputs side by side\n",
    "    # output projection\n",
    "    y = self.c_proj(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "    self.n_heads = n_heads\n",
    "    self.head_size = head_size\n",
    "    bound = 1 / (self.head_size**0.5)\n",
    "    self.queries = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.keys = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.values = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.proj = nn.Linear(n_heads * head_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    B, T, C = x.shape\n",
    "\n",
    "    x = x.unsqueeze(1).expand((B, self.n_heads, T, C))\n",
    "\n",
    "    Q = x @ self.queries  # (B, n_heads, T, head_size)\n",
    "    K = x @ self.keys  # (B, n_heads, T, head_size)\n",
    "    V = x @ self.values  # (B, n_heads, T, head_size)\n",
    "    mask = Tensor.ones((T, T), requires_grad=False).tril()\n",
    "    attented_embeds = Tensor.scaled_dot_product_attention(\n",
    "      Q, K, V, attn_mask=mask\n",
    "    )  # (B, n_heads, T, head_size)\n",
    "    concatenated_embeds = attented_embeds.reshape(\n",
    "      (B, T, self.n_heads * self.head_size)\n",
    "    )  # (B, T, n_heads * head_size)\n",
    "    out = self.proj(concatenated_embeds)  # (B, T, C)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    return x.sequential([self.fc1, Tensor.gelu, self.fc2])\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "  # def __init__(self, embed_size: int, n_heads: int, block_size: int) -> None:\n",
    "    self.ln1 = nn.LayerNorm(embed_size)\n",
    "    self.attn = Attention(embed_size, n_heads, head_size)\n",
    "    # self.attn = CausalSelfAttention(embed_size, n_heads, block_size)\n",
    "    self.ln2 = nn.LayerNorm(embed_size)\n",
    "    self.mlp = MLP(embed_size, 4 * embed_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    x = x + self.attn(self.ln1(x))\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(\n",
    "    self,\n",
    "    block_size: int,\n",
    "    vocab_size: int,\n",
    "    embed_size: int,\n",
    "    n_layers: int,\n",
    "    n_heads: int,\n",
    "    head_size: int,\n",
    "  ) -> None:\n",
    "    self.block_size = block_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "    self.h = [TransformerBlock(embed_size, n_heads, head_size) for _ in range(n_layers)]\n",
    "    # self.h = [TransformerBlock(embed_size, n_heads, block_size) for _ in range(n_layers)]\n",
    "    self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    assert len(x.shape) == 2 and x.shape[1] == self.block_size\n",
    "    B, T = x.shape\n",
    "    embed = self.token_embed(x) + self.pos_embed(Tensor.arange(T))\n",
    "    logits = embed.sequential(self.h + [self.lm_head])\n",
    "    return logits\n",
    "\n",
    "  def loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "    logits = self(x)\n",
    "    loss = logits.sparse_categorical_crossentropy(y)\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, x: Tensor, n: int = 500) -> Tensor:\n",
    "    assert len(x.shape) == 1 and x.shape[0] == self.block_size\n",
    "    x = x.unsqueeze(0)\n",
    "    for _ in range(n):\n",
    "      logits = self(x[:, -self.block_size :])\n",
    "      p = logits[:, -1].softmax().squeeze(0)\n",
    "      next_token = np.random.choice(self.vocab_size, p=p.numpy())\n",
    "      x = x.cat(Tensor([[next_token]]), dim=1)\n",
    "    return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25289793"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 32  # 16\n",
    "embed_size = 512\n",
    "n_heads = 4  # 4\n",
    "head_size = embed_size // n_heads\n",
    "transformer = Transformer(\n",
    "  block_size=block_size,\n",
    "  vocab_size=vocab_size,\n",
    "  embed_size=embed_size,\n",
    "  n_layers=8,  # 4\n",
    "  n_heads=n_heads,\n",
    "  head_size=head_size,\n",
    ")\n",
    "sum(p.numel() for p in nn.state.get_parameters(transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.AdamW(nn.state.get_parameters(transformer))\n",
    "batch_size = 16  # 128\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "  optim.zero_grad()\n",
    "  x_samples, y_samples = get_batch(train_data, batch_size, block_size)\n",
    "  _, loss = transformer.loss(x_samples, y_samples)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 4.34, acc 13.09%\n",
      "step 100, loss 0.80, acc 85.35%\n",
      "step 200, loss 0.13, acc 97.66%\n",
      "step 300, loss 0.09, acc 98.24%\n",
      "step 400, loss 0.08, acc 97.66%\n",
      "step 500, loss 0.10, acc 97.46%\n",
      "step 600, loss 0.09, acc 98.05%\n",
      "step 700, loss 0.09, acc 97.27%\n",
      "step 800, loss 0.10, acc 98.05%\n",
      "step 900, loss 0.10, acc 97.46%\n",
      "step 1000, loss 0.13, acc 97.66%\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for step in range(1, 1001):\n",
    "  loss = train_step().item()\n",
    "  losses.append(loss)\n",
    "  if step == 1 or step % 100 == 0:\n",
    "    with Tensor.inference_mode():\n",
    "      x_samples, y_samples = get_batch(test_data, batch_size, block_size)\n",
    "      acc = (transformer(x_samples).argmax(axis=-1) == y_samples).mean().item()\n",
    "      print(f\"step {step}, loss {loss:.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mFirst Citizen:\n",
      "Before we proceed\u001b[0m,\n",
      "Ahe .e my heo ? o me s y th, fhste?\n",
      "\n",
      "Aw hef nare cher hald Ye s ic ded w your P the ro d verthervel cd I dovelf d the hithered d  coer der der hio hhin he kis des sts drow heced deecker be ste sed-l ded ces he\n",
      "A?OA,ONOOKIO:\n",
      "Asucl\n",
      "A the IOcea uithe ke thowwowwh kaed eravetherthe,\n",
      "WheUo fre I thas st he the s nof se dor certher\n",
      "\n",
      "WIN:\n",
      "W chivathe the he pe r ver ghougherald s theathe ce the nond dd,\n",
      " he vieele be herd b ath fooy frd fo  no\n",
      "Wheds.\n",
      "ARK:AR3OAA I I:\n",
      "A in wutcer ditht td y herd he hest we\n",
      "A mevert hu\n",
      "AncHere ait lfd wr fso-f bkers,\n",
      "B hin kerbe di s? se ceul ver Bal thir shers cer ther\n",
      "Thel fid ce pe bey than heowhem tow bt bichcer fove fececthe kerrd cerde of  nove shel Jde\n",
      "WONGA:\n",
      "A mumy waow f m ul d w  nce ther  haot detI\n",
      "\n",
      "YowI\n",
      ":A s rond ha herle churd bee n in heu arp dr he sheng,\n",
      "W heace py hhel her I cithe kel\n",
      "A dnve\n",
      "An they sis sovaRI:\n",
      "A d beAHHELLNABO:: thia mer he l ch fo- fke wh.\n",
      "\n",
      "ANHAABBABBBABOINBBBABBBBBBBBBABBOA:OOABAOBBBEBAABAAAOA:AOA:\n",
      "O:\n",
      "Ahay IOBAOOOAANAIOAROABA\n"
     ]
    }
   ],
   "source": [
    "text = decode(transformer.generate(data[:block_size], n=1000).numpy())\n",
    "print(\"\\033[92m\" + text[:block_size] + \"\\033[0m\" + text[block_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
