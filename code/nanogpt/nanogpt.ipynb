{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import os\n",
    "# os.environ['JIT'] = '2'\n",
    "Device.DEFAULT = \"GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"shakespeare.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 chars\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), \"chars\")\n",
    "print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, \"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line_char = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = lambda s: [vocab.index(c) for c in s]\n",
    "decode = lambda l: \"\".join([vocab[i] for i in l])\n",
    "\n",
    "decode(encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(encode(corpus))\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data: Tensor, batch_size, block_size):\n",
    "  indices = Tensor.randint((batch_size,), high=len(data) - block_size).reshape(\n",
    "    (batch_size, 1)\n",
    "  ) + Tensor.arange(block_size)\n",
    "  return data[indices], data[indices + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (4, 8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(train_data, batch_size=4, block_size=8)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YORK:\\nLi', 'at doth ', 'and my s', 'els have']\n",
      "['ORK:\\nLit', 't doth b', 'nd my su', 'ls have ']\n"
     ]
    }
   ],
   "source": [
    "print([decode(row) for row in x.numpy()])\n",
    "print([decode(row) for row in y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "    self.n_heads = n_heads\n",
    "    self.head_size = head_size\n",
    "    bound = 1 / (self.head_size**0.5)\n",
    "    self.queries = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.keys = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.values = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    B, T, C = x.shape\n",
    "\n",
    "    x = x.unsqueeze(1).expand((B, self.n_heads, T, C))\n",
    "\n",
    "    Q = x @ self.queries  # (B, n_heads, T, head_size)\n",
    "    K = x @ self.keys  # (B, n_heads, T, head_size)\n",
    "    V = x @ self.values  # (B, n_heads, T, head_size)\n",
    "    attented_embeds = Tensor.scaled_dot_product_attention(\n",
    "      Q, K, V, attn_mask=Tensor.ones((T, T), requires_grad=False).tril()\n",
    "    )  # noqa: F401, (B, n_heads, T, head_size)\n",
    "    concatenated_embeds = attented_embeds.reshape((B, T, self.n_heads * self.head_size))  # noqa: F401, (B, T, n_heads * head_size)\n",
    "    return concatenated_embeds\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "    self.attn = Attention(embed_size, n_heads, head_size)\n",
    "    self.out_proj = nn.Linear(n_heads * head_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    return x.sequential([self.attn, self.out_proj, Tensor.gelu])\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(\n",
    "    self,\n",
    "    block_size: int,\n",
    "    vocab_size: int,\n",
    "    embed_size: int,\n",
    "    n_layers: int,\n",
    "    n_heads: int,\n",
    "    head_size: int,\n",
    "  ) -> None:\n",
    "    self.block_size = block_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "    self.h = [TransformerBlock(embed_size, n_heads, head_size) for _ in range(n_layers)]\n",
    "    self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    assert len(x.shape) == 2 and x.shape[1] == self.block_size\n",
    "    B, T = x.shape\n",
    "    embed = self.token_embed(x) + self.pos_embed(Tensor.arange(T))\n",
    "    logits = embed.sequential(self.h + [self.lm_head])\n",
    "    return logits\n",
    "\n",
    "  def loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "    logits = self(x)\n",
    "    loss = logits.sparse_categorical_crossentropy(y)\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, x: Tensor, n: int = 500) -> Tensor:\n",
    "    assert len(x.shape) == 1 and x.shape[0] == self.block_size\n",
    "    x = x.unsqueeze(0)\n",
    "    for _ in range(n):\n",
    "      logits = self(x[:, -self.block_size :])\n",
    "      p = logits[:, -1].softmax().squeeze(0)\n",
    "      next_token = np.random.choice(self.vocab_size, p=p.numpy())\n",
    "      x = x.cat(Tensor([[next_token]]), dim=1)\n",
    "    return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2172993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 16 # 32\n",
    "embed_size = 512\n",
    "n_heads = 4\n",
    "head_size = embed_size // n_heads\n",
    "transformer = Transformer(\n",
    "  block_size=block_size,\n",
    "  vocab_size=vocab_size,\n",
    "  embed_size=embed_size,\n",
    "  n_layers=2, # 4\n",
    "  n_heads=n_heads,\n",
    "  head_size=head_size,\n",
    ")\n",
    "sum(p.numel() for p in nn.state.get_parameters(transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.AdamW(nn.state.get_parameters(transformer))\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "  optim.zero_grad()\n",
    "  x_samples, y_samples = get_batch(train_data, batch_size, block_size)\n",
    "  _, loss = transformer.loss(x_samples, y_samples)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 4.17, acc 3.66%\n",
      "step 250, loss 1.40, acc 46.53%\n",
      "step 500, loss 1.10, acc 48.14%\n",
      "step 750, loss 0.99, acc 56.05%\n",
      "step 1000, loss 0.83, acc 60.16%\n",
      "step 1250, loss 0.80, acc 59.91%\n",
      "step 1500, loss 0.79, acc 59.86%\n",
      "step 1750, loss 0.79, acc 59.91%\n",
      "step 2000, loss 0.78, acc 59.81%\n",
      "step 2250, loss 0.77, acc 60.35%\n",
      "step 2500, loss 0.77, acc 60.35%\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for step in range(1, 2501):\n",
    "  loss = train_step().item()\n",
    "  losses.append(loss)\n",
    "  if step == 1 or step % 250 == 0:\n",
    "    with Tensor.inference_mode():\n",
    "      x_samples, y_samples = get_batch(test_data, batch_size, block_size)\n",
    "      acc = (transformer(x_samples).argmax(axis=-1) == y_samples).mean().item()\n",
    "      print(f\"step {step}, loss {loss:.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mFirst Citizen:\n",
      "B\u001b[0mut isinin far my ving has bune aremy bagond sos tame! the of nd, mand's afout qurad bearlle youcantem! I his rasiases and that thadsof win nor soonmeruproin tor, ang wh m this sagess svand's.\n",
      "Then you pow thand.\n",
      "\n",
      "PARTUS:\n",
      "HTRY say me dearscous sad; ied, forathem wous.\n",
      "\n",
      "MESN RY:\n",
      "Fit your abtiexs slnge meads notlat thou see makiln thys his tips\n",
      "\n",
      "FORE:\n",
      "Cof berk titer otat ruse that ponthourstoru d:\n",
      "gourveadoth mout king cond\n",
      "besty hourinstses soveprimaststread tise my thowseveve with apawh to gughll\n"
     ]
    }
   ],
   "source": [
    "text = decode(transformer.generate(data[:block_size]).numpy())\n",
    "print(\"\\033[92m\" + text[:block_size] + \"\\033[0m\" + text[block_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
