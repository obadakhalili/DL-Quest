{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"shakespeare.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 chars\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), \"chars\")\n",
    "print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, \"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line_char = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = lambda s: [vocab.index(c) for c in s]\n",
    "decode = lambda l: \"\".join([vocab[i] for i in l])\n",
    "\n",
    "decode(encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(encode(corpus))\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data: Tensor, batch_size, block_size):\n",
    "  indices = Tensor.randint((batch_size,), high=len(data) - block_size).reshape(\n",
    "    (batch_size, 1)\n",
    "  ) + Tensor.arange(block_size)\n",
    "  return data[indices], data[indices + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (4, 8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(train_data, batch_size=4, block_size=8)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' come do', 'h\\nBut th', 'UMBERLAN', 'll money']\n",
      "['come dow', '\\nBut the', 'MBERLAND', 'l money;']\n"
     ]
    }
   ],
   "source": [
    "print([decode(row) for row in x.numpy()])\n",
    "print([decode(row) for row in y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "    self.n_heads = n_heads\n",
    "    self.head_size = head_size\n",
    "    bound = 1 / math.sqrt(self.head_size)\n",
    "    self.queries = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.keys = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.values = Tensor.uniform(\n",
    "      n_heads, embed_size, self.head_size, low=-bound, high=bound\n",
    "    )\n",
    "    self.proj = nn.Linear(n_heads * head_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    B, T, C = x.shape\n",
    "\n",
    "    x = x.unsqueeze(1).expand((B, self.n_heads, T, C))\n",
    "\n",
    "    Q = x @ self.queries  # (B, n_heads, T, head_size)\n",
    "    K = x @ self.keys  # (B, n_heads, T, head_size)\n",
    "    V = x @ self.values  # (B, n_heads, T, head_size)\n",
    "\n",
    "    attn = Q @ K.transpose(-2, -1) / math.sqrt(self.head_size)  # (B, n_heads, T, T)\n",
    "    mask = Tensor.ones((T, T), requires_grad=False).tril()\n",
    "    attn = attn.masked_fill(mask == 0, float(\"-inf\"))  # (B, n_heads, T, T)\n",
    "    attn = attn.softmax()  # (B, n_heads, T, T)\n",
    "\n",
    "    y = attn @ V  # (B, n_heads, T, head_size)\n",
    "    y = y.transpose(1, 2).reshape((B, T, -1))  # (B, T, n_heads * head_size)\n",
    "    y = self.proj(y)  # (B, T, C)\n",
    "    return y\n",
    "\n",
    "\n",
    "class MLP:\n",
    "  def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    return x.sequential([self.fc1, Tensor.gelu, self.fc2])\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "  def __init__(self, embed_size: int, n_heads: int, head_size: int) -> None:\n",
    "    self.ln1 = nn.LayerNorm(embed_size)\n",
    "    self.attn = Attention(embed_size, n_heads, head_size)\n",
    "    self.ln2 = nn.LayerNorm(embed_size)\n",
    "    self.mlp = MLP(embed_size, 4 * embed_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    x = x + self.attn(self.ln1(x))\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(\n",
    "    self,\n",
    "    block_size: int,\n",
    "    vocab_size: int,\n",
    "    embed_size: int,\n",
    "    n_layers: int,\n",
    "    n_heads: int,\n",
    "    head_size: int,\n",
    "  ) -> None:\n",
    "    self.block_size = block_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "    self.h = [TransformerBlock(embed_size, n_heads, head_size) for _ in range(n_layers)]\n",
    "    self.ln = nn.LayerNorm(embed_size)\n",
    "    self.lm_head = nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    assert len(x.shape) == 2 and x.shape[1] == self.block_size\n",
    "    B, T = x.shape\n",
    "    embed = self.token_embed(x) + self.pos_embed(Tensor.arange(T))\n",
    "    logits = embed.sequential(self.h + [self.ln, self.lm_head])\n",
    "    return logits\n",
    "\n",
    "  def loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "    logits = self(x)\n",
    "    loss = logits.sparse_categorical_crossentropy(y)\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, x: Tensor, n: int = 500) -> Tensor:\n",
    "    assert len(x.shape) == 1 and x.shape[0] == self.block_size\n",
    "    with Tensor.inference_mode():\n",
    "      x = x.unsqueeze(0)\n",
    "      for _ in range(n):\n",
    "        logits = self(x[:, -self.block_size :])\n",
    "        p = logits[:, -1].softmax().squeeze(0)\n",
    "        next_token = np.random.choice(self.vocab_size, p=p.numpy())\n",
    "        x = x.cat(Tensor([[next_token]]), dim=1)\n",
    "      return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6378496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 128\n",
    "embed_size = 256\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "head_size = embed_size // n_heads\n",
    "transformer = Transformer(\n",
    "  block_size, vocab_size, embed_size, n_layers, n_heads, head_size\n",
    ")\n",
    "sum(p.numel() for p in nn.state.get_parameters(transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.AdamW(nn.state.get_parameters(transformer))\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "  optim.zero_grad()\n",
    "  x_samples, y_samples = get_batch(train_data, batch_size, block_size)\n",
    "  _, loss = transformer.loss(x_samples, y_samples)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 4.21, acc 15.15%\n",
      "step 100, loss 2.54, acc 26.04%\n",
      "step 200, loss 2.42, acc 28.61%\n",
      "step 300, loss 2.21, acc 34.20%\n",
      "step 400, loss 1.88, acc 42.11%\n",
      "step 500, loss 1.74, acc 43.99%\n",
      "step 600, loss 1.62, acc 46.55%\n",
      "step 700, loss 1.53, acc 49.46%\n",
      "step 800, loss 1.50, acc 49.50%\n",
      "step 900, loss 1.45, acc 50.18%\n",
      "step 1000, loss 1.43, acc 51.09%\n",
      "step 1100, loss 1.36, acc 52.84%\n",
      "step 1200, loss 1.31, acc 52.09%\n",
      "step 1300, loss 1.33, acc 53.33%\n",
      "step 1400, loss 1.32, acc 53.75%\n",
      "step 1500, loss 1.27, acc 53.75%\n",
      "step 1600, loss 1.23, acc 53.82%\n",
      "step 1700, loss 1.23, acc 54.99%\n",
      "step 1800, loss 1.25, acc 54.77%\n",
      "step 1900, loss 1.18, acc 53.77%\n",
      "step 2000, loss 1.21, acc 54.21%\n",
      "step 2100, loss 1.19, acc 54.54%\n",
      "step 2200, loss 1.17, acc 54.59%\n",
      "step 2300, loss 1.18, acc 55.77%\n",
      "step 2400, loss 1.13, acc 55.18%\n",
      "step 2500, loss 1.14, acc 54.68%\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for step in range(1, 2501):\n",
    "  loss = train_step().item()\n",
    "  losses.append(loss)\n",
    "  if step == 1 or step % 100 == 0:\n",
    "    with Tensor.inference_mode():\n",
    "      x_samples, y_samples = get_batch(test_data, batch_size, block_size)\n",
    "      acc = (transformer(x_samples).argmax(axis=-1) == y_samples).mean().item()\n",
    "      print(f\"step {step}, loss {loss:.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mFirst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to \u001b[0mbe grieved\n",
      "I merciful, mouth, and shall toke of reputation.\n",
      "\n",
      "ROMEO:\n",
      "No, be Ever, bear him to be married\n",
      "That break and affecting her just.\n",
      "\n",
      "GLOUCESTER:\n",
      "Bless I stay, when Gloucester wings and be so,\n",
      "For reputy but at an Ireland.\n",
      "\n",
      "KING RICHARD III:\n",
      "Away, then, be my merits! Upot live tears?\n",
      "\n",
      "Clown:\n",
      "My successive can yield as these highness of me;\n",
      "My worship for Claudio. Hark! 'tis Cup rot,\n",
      "What they may listore Claudio, when the sulder?\n",
      "\n",
      "Second Citizen:\n",
      "Why, my lord, I protetly day!\n",
      "\n",
      "First Citizen:\n",
      "Not stand not proves, come, nor own rember eyes\n",
      "In march, for thy shoulders them for sleep,\n",
      "A little lambs-ating the old light,\n",
      "When he was speaking, mine honour, comes hither. Notive:\n",
      "\n",
      "JOHN OF CARGARD II:\n",
      "Why not knigh me in Rome,\n",
      "Repent may complant them he strike\n",
      "With successful plate confess; swift\n",
      "Is seem at our hard it, if he cannot\n",
      "Had kill'd the arms of heaven. Doth thy robe\n",
      "The iglet wins and his brother; when the gates\n",
      "The plays of whore. I in hand thy godden gear;\n",
      "Or something live\n"
     ]
    }
   ],
   "source": [
    "text = decode(transformer.generate(data[:block_size], 1000).numpy())\n",
    "print(\"\\033[92m\" + text[:block_size] + \"\\033[0m\" + text[block_size:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
