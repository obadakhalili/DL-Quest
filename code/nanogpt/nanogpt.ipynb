{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit, Device\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/tinygrad/tinygrad/issues/5408\n",
    "# import os\n",
    "# os.environ['JIT'] = '2'\n",
    "Device.DEFAULT = \"GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"shakespeare.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 chars\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), \"chars\")\n",
    "print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, \"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line_char = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = lambda s: [vocab.index(c) for c in s]\n",
    "decode = lambda l: \"\".join([vocab[i] for i in l])\n",
    "\n",
    "decode(encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(encode(corpus))\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "test_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data: Tensor, batch_size, block_size):\n",
    "  indices = Tensor.randint((batch_size,), high=len(data) - block_size).reshape(\n",
    "    (batch_size, 1)\n",
    "  ) + Tensor.arange(block_size)\n",
    "  return data[indices], data[indices + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 8), (4, 8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch(train_data, batch_size=4, block_size=8)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nHath ye', 'ich Capu', '-shrunk.', 'oo near,']\n",
      "['Hath yet', 'ch Capul', 'shrunk.\\n', 'o near, ']\n"
     ]
    }
   ],
   "source": [
    "print([decode(row) for row in x.numpy()])\n",
    "print([decode(row) for row in y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self, n_embd: int, n_head: int):\n",
    "    assert n_embd % n_head == 0\n",
    "    # key, query, value projections for all heads, but in a batch\n",
    "    self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "    # regularization\n",
    "    self.n_head = n_head\n",
    "    self.n_embd = n_embd\n",
    "\n",
    "  def __call__(self, x: Tensor):\n",
    "    B, T, C = x.shape\n",
    "    qkv = self.c_attn(x)\n",
    "    q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "    # manual implementation of attention\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    mask = Tensor.ones(T, T).tril()\n",
    "    att = att.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    att = att.softmax()\n",
    "    y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    y = y.transpose(1, 2).view(B, T, C)  # re-assemble all head outputs side by side\n",
    "    # output projection\n",
    "    y = self.c_proj(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "class MLP:\n",
    "  def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    return x.sequential([self.fc1, Tensor.relu, self.fc2])\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "  def __init__(self, embed_size: int, n_heads: int) -> None:\n",
    "    self.ln1 = nn.LayerNorm(embed_size)\n",
    "    self.attn = Attention(embed_size, n_heads)\n",
    "    self.ln2 = nn.LayerNorm(embed_size)\n",
    "    self.mlp = MLP(embed_size, 4 * embed_size, embed_size)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    x = x + self.attn(self.ln1(x))\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "  def __init__(\n",
    "    self,\n",
    "    block_size: int,\n",
    "    vocab_size: int,\n",
    "    embed_size: int,\n",
    "    n_layers: int,\n",
    "    n_heads: int,\n",
    "  ) -> None:\n",
    "    self.block_size = block_size\n",
    "    self.vocab_size = vocab_size\n",
    "    self.token_embed = nn.Embedding(vocab_size, embed_size)\n",
    "    self.pos_embed = nn.Embedding(block_size, embed_size)\n",
    "    self.h = [TransformerBlock(embed_size, n_heads) for _ in range(n_layers)]\n",
    "    self.ln_f = nn.LayerNorm(embed_size)\n",
    "    self.lm_head = nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "  def __call__(self, x: Tensor) -> Tensor:\n",
    "    assert len(x.shape) == 2 and x.shape[1] == self.block_size\n",
    "    B, T = x.shape\n",
    "    embed = self.token_embed(x) + self.pos_embed(Tensor.arange(T))\n",
    "    logits = self.lm_head(self.ln_f(embed.sequential(self.h)))\n",
    "    return logits\n",
    "\n",
    "  def loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "    logits = self(x)\n",
    "    loss = logits.sparse_categorical_crossentropy(y)\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, x: Tensor, n: int = 500) -> Tensor:\n",
    "    assert len(x.shape) == 1 and x.shape[0] == self.block_size\n",
    "    x = x.unsqueeze(0)\n",
    "    for _ in range(n):\n",
    "      logits = self(x[:, -self.block_size :])\n",
    "      p = logits[:, -1].softmax().squeeze(0)\n",
    "      next_token = np.random.choice(self.vocab_size, p=p.numpy())\n",
    "      x = x.cat(Tensor([[next_token]]), dim=1)\n",
    "    return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814080"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 32\n",
    "embed_size = 128\n",
    "n_heads = 4\n",
    "transformer = Transformer(\n",
    "  block_size=block_size,\n",
    "  vocab_size=vocab_size,\n",
    "  embed_size=embed_size,\n",
    "  n_layers=4,\n",
    "  n_heads=n_heads,\n",
    ")\n",
    "sum(p.numel() for p in nn.state.get_parameters(transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.AdamW(nn.state.get_parameters(transformer))\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "  optim.zero_grad()\n",
    "  x_samples, y_samples = get_batch(train_data, batch_size, block_size)\n",
    "  _, loss = transformer.loss(x_samples, y_samples)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 4.31, acc 15.06%\n",
      "step 100, loss 2.32, acc 32.30%\n",
      "step 200, loss 2.02, acc 39.21%\n",
      "step 300, loss 1.83, acc 42.92%\n",
      "step 400, loss 1.69, acc 45.21%\n",
      "step 500, loss 1.66, acc 44.24%\n",
      "step 600, loss 1.63, acc 46.88%\n",
      "step 700, loss 1.57, acc 47.88%\n",
      "step 800, loss 1.53, acc 48.12%\n",
      "step 900, loss 1.51, acc 48.88%\n",
      "step 1000, loss 1.50, acc 52.22%\n",
      "step 1100, loss 1.46, acc 51.15%\n",
      "step 1200, loss 1.45, acc 49.88%\n",
      "step 1300, loss 1.49, acc 51.54%\n",
      "step 1400, loss 1.46, acc 51.05%\n",
      "step 1500, loss 1.44, acc 50.32%\n",
      "step 1600, loss 1.52, acc 51.51%\n",
      "step 1700, loss 1.38, acc 50.81%\n",
      "step 1800, loss 1.45, acc 51.27%\n",
      "step 1900, loss 1.45, acc 51.73%\n",
      "step 2000, loss 1.45, acc 51.54%\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for step in range(1, 2001):\n",
    "  loss = train_step().item()\n",
    "  losses.append(loss)\n",
    "  if step == 1 or step % 100 == 0:\n",
    "    with Tensor.inference_mode():\n",
    "      x_samples, y_samples = get_batch(test_data, batch_size, block_size)\n",
    "      acc = (transformer(x_samples).argmax(axis=-1) == y_samples).mean().item()\n",
    "      print(f\"step {step}, loss {loss:.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mFirst Citizen:\n",
      "Before we proceed\u001b[0m itself; the that good see,\n",
      "That's envish you, leave you pierce thine her bloody cometry.\n",
      "Thou shalt ones grace; and skull what woo more sapneeling by and they do no worrup in these feelss ensurnes to keep thete alse up.\n",
      "What I may so as Tybalt thou Hereford, 'twixe tells allsion to her of warrant:\n",
      "Come, let to me my son, I am now;\n",
      "And crow she wave as lips; and to the king,\n",
      "Which said I coung stange all: hence his marriage, never will, could all these\n",
      "Must be sudden shall staff once hath foolish'd.\n",
      "\n",
      "HENRY BOLINGS:\n",
      "Calary as your paunt great frot, you seek your prosition of your edwar:\n",
      "So all, pupon me ear by your true,\n",
      "The professial tyrange of Will'st;\n",
      "The hands, that stay Marcius that gases hath Roman;\n",
      "For me leave you? herefore I never 'tis beg and\n",
      "Diving that amazes we are pale judge!\n",
      "If it is namedhes and that will\n",
      "Here are of the own creature out upon our spirite\n",
      "To him with youtestle lap the widow,\n",
      "In wead set me to love a shepherd;\n",
      "And you must not living did scee;\n",
      "Confess to \n"
     ]
    }
   ],
   "source": [
    "text = decode(transformer.generate(data[:block_size], 1000).numpy())\n",
    "print(\"\\033[92m\" + text[:block_size] + \"\\033[0m\" + text[block_size:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
