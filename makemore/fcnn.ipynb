{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"./names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKEN = \".\"\n",
    "vocab = [SPECIAL_TOKEN] + [chr(unicode) for unicode in range(ord(\"a\"), ord(\"a\") + 26)]\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "\n",
    "for name in names:\n",
    "    name_chars = [SPECIAL_TOKEN] + list(name) + [SPECIAL_TOKEN]\n",
    "    for char_a, char_b in zip(name_chars, name_chars[1:]):\n",
    "        x.append(vocab.index(char_a))\n",
    "        y.append(vocab.index(char_b))\n",
    "\n",
    "split = int(0.8 * len(x))\n",
    "X_train, y_train, X_test, y_test = (\n",
    "    Tensor(x[:split]).one_hot(vocab_size),\n",
    "    Tensor(y[:split]),\n",
    "    Tensor(x[split:]).one_hot(vocab_size),\n",
    "    Tensor(y[split:]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "fcnn = FCNN(vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.Adam(nn.state.get_parameters(fcnn))\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# def nll_loss(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "#     return -y_pred.log()[y_true].mean()\n",
    "\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train_step():\n",
    "    optim.zero_grad()\n",
    "    samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "    X_samples, y_samples = X_train[samples], y_train[samples]\n",
    "    # loss = nll_loss(fcnn(X_samples), y_samples).backward()\n",
    "    loss = fcnn(X_samples).sparse_categorical_crossentropy(y_samples).backward()\n",
    "    optim.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 3.30, acc 3.40%\n",
      "step 100, loss 3.17, acc 7.89%\n",
      "step 200, loss 3.10, acc 14.23%\n",
      "step 300, loss 3.00, acc 15.22%\n",
      "step 400, loss 2.87, acc 17.40%\n",
      "step 500, loss 2.86, acc 18.61%\n",
      "step 600, loss 2.76, acc 18.61%\n",
      "step 700, loss 2.66, acc 19.41%\n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 701):\n",
    "    loss = train_step()\n",
    "    if step == 1 or step % 100 == 0:\n",
    "        Tensor.training = False\n",
    "        acc = (fcnn(X_test).argmax(axis=1) == y_test).mean().item()\n",
    "        print(f\"step {step}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(starting_chars=\"\", max_len=20):\n",
    "    word = SPECIAL_TOKEN + starting_chars\n",
    "    while len(word[1:]) < max_len:\n",
    "        x = Tensor([vocab.index(word[-1])]).one_hot(vocab_size)\n",
    "        y = fcnn(x).argmax().item()\n",
    "        if y == vocab.index(SPECIAL_TOKEN):\n",
    "            break\n",
    "        word += vocab[y]\n",
    "    return word[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "be\n",
      "ca\n",
      "da\n",
      "e\n",
      "fa\n",
      "ga\n",
      "h\n",
      "i\n",
      "ja\n",
      "ka\n",
      "le\n",
      "ma\n",
      "n\n",
      "on\n",
      "pa\n",
      "qa\n",
      "ri\n",
      "sa\n",
      "t\n",
      "u\n",
      "vi\n",
      "wa\n",
      "x\n",
      "ya\n",
      "za\n"
     ]
    }
   ],
   "source": [
    "for item in vocab:\n",
    "    print(generate_word(item.replace(SPECIAL_TOKEN, \"\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
